{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."
      ],
      "metadata": {
        "id": "lhpqQ3AVYQh7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3VXMWUdVy4M",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** To make it easier for us to manage datasets, images and models we create a `HOME` constant."
      ],
      "metadata": {
        "id": "AIcGNEx3YmKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(\"HOME:\", HOME)"
      ],
      "metadata": {
        "id": "-ipjkvtAYSiR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Grounding DINO and Segment Anything Model\n",
        "\n",
        "Our project will use two groundbreaking designs - [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) - for zero-shot detection and [Segment Anything Model (SAM)](https://github.com/facebookresearch/segment-anything) - for converting boxes into segmentations. We have to install them first.\n"
      ],
      "metadata": {
        "id": "s3VuH-MxY5ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "%cd {HOME}/GroundingDINO\n",
        "!git checkout -q 57535c5a79791cb76e36fdb64975271354f10251\n",
        "!pip install -q -e ."
      ],
      "metadata": {
        "id": "mnlQNpMpwaP3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "\n",
        "import sys\n",
        "!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'"
      ],
      "metadata": {
        "id": "vGwLQ7mBw9WJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** To glue all the elements of our demo together we will use the [`supervision`](https://github.com/roboflow/supervision) pip package, which will help us **process, filter and visualize our detections as well as to save our dataset**. A lower version of the `supervision` was installed with Grounding DINO. However, in this demo we need the functionality introduced in the latest versions. Therefore, we uninstall the current `supervsion` version and install version `0.6.0`.\n",
        "\n"
      ],
      "metadata": {
        "id": "vrK8geyHt6JV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y supervision\n",
        "!pip install -q supervision\n",
        "\n",
        "import supervision as sv\n",
        "print(sv.__version__)"
      ],
      "metadata": {
        "id": "NxU6_aWCNSFq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** At the end of the tutorial we will upload our annotations to [Roboflow](roboflow.com). To automate this process with the API, let's install the `roboflow` pip package."
      ],
      "metadata": {
        "id": "U08MXHCEluJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q roboflow\n",
        "!pip install --upgrade gradio"
      ],
      "metadata": {
        "id": "QevJDQsuu492",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Grounding DINO Model Weights\n",
        "\n",
        "To run Grounding DINO we need two files - configuration and model weights. The configuration file is part of the [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) repository, which we have already cloned. The weights file, on the other hand, we need to download. We write the paths to both files to the `GROUNDING_DINO_CONFIG_PATH` and `GROUNDING_DINO_CHECKPOINT_PATH` variables and verify if the paths are correct and the files exist on disk."
      ],
      "metadata": {
        "id": "GGV-pHiDgZs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "GROUNDING_DINO_CONFIG_PATH = os.path.join(HOME, \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
        "print(GROUNDING_DINO_CONFIG_PATH, \"; exist:\", os.path.isfile(GROUNDING_DINO_CONFIG_PATH))"
      ],
      "metadata": {
        "id": "qtMIfEt4gdUX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "!mkdir -p {HOME}/weights\n",
        "%cd {HOME}/weights\n",
        "\n",
        "!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth"
      ],
      "metadata": {
        "id": "p-Eoargcg3Bp",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "GROUNDING_DINO_CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"groundingdino_swint_ogc.pth\")\n",
        "print(GROUNDING_DINO_CHECKPOINT_PATH, \"; exist:\", os.path.isfile(GROUNDING_DINO_CHECKPOINT_PATH))"
      ],
      "metadata": {
        "id": "BVkviPClh0Nh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Segment Anything Model (SAM) Weights\n",
        "\n",
        "As with Grounding DINO, in order to run SAM we need a weights file, which we must first download. We write the path to local weight file to `SAM_CHECKPOINT_PATH` variable and verify if the path is correct and the file exist on disk."
      ],
      "metadata": {
        "id": "hJqh1YOwdahU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "!mkdir -p {HOME}/weights\n",
        "%cd {HOME}/weights\n",
        "\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ],
      "metadata": {
        "id": "XUEOO4bXdFJ4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "SAM_CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
        "print(SAM_CHECKPOINT_PATH, \"; exist:\", os.path.isfile(SAM_CHECKPOINT_PATH))"
      ],
      "metadata": {
        "id": "ZsNiAQqTdM2c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load models"
      ],
      "metadata": {
        "id": "ByAcxA43a484"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "xJzLz7oUZ0e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Grounding DINO Model"
      ],
      "metadata": {
        "id": "98JPP_icc5hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}/GroundingDINO\n",
        "\n",
        "from groundingdino.util.inference import Model\n",
        "\n",
        "grounding_dino_model = Model(model_config_path=GROUNDING_DINO_CONFIG_PATH, model_checkpoint_path=GROUNDING_DINO_CHECKPOINT_PATH)"
      ],
      "metadata": {
        "id": "odl3HAdWc5Gz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Segment Anything Model (SAM)"
      ],
      "metadata": {
        "id": "DBTCh84cczrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAM_ENCODER_VERSION = \"vit_h\""
      ],
      "metadata": {
        "id": "p23VWJ2adrK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "sam = sam_model_registry[SAM_ENCODER_VERSION](checkpoint=SAM_CHECKPOINT_PATH).to(device=DEVICE)\n",
        "sam_predictor = SamPredictor(sam)"
      ],
      "metadata": {
        "id": "-PBgnrmgcLSW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f\"{HOME}/data\""
      ],
      "metadata": {
        "id": "3L6S4MeLLTfS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "!mkdir {HOME}/data\n",
        "%cd {HOME}/data\n",
        "\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog.jpeg\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-2.jpeg\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-3.jpeg\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-4.jpeg\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-5.jpeg\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-6.jpeg\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-7.jpeg\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-8.jpeg"
      ],
      "metadata": {
        "id": "XB-oJV2Gj6pX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explainable User Interface"
      ],
      "metadata": {
        "id": "m9ZLrPLcoqlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from typing import List\n",
        "from segment_anything import SamPredictor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def enhance_class_name(class_names: List[str]) -> List[str]:\n",
        "    return [\n",
        "        f\"all {class_name}s\"\n",
        "        for class_name\n",
        "        in class_names\n",
        "    ]\n",
        "\n",
        "def segment(sam_predictor: SamPredictor, image: np.ndarray, xyxy: np.ndarray) -> np.ndarray:\n",
        "    sam_predictor.set_image(image)\n",
        "    result_masks = []\n",
        "    for box in xyxy:\n",
        "        masks, scores, logits = sam_predictor.predict(\n",
        "            box=box,\n",
        "            multimask_output=True\n",
        "        )\n",
        "        index = np.argmax(scores)\n",
        "        result_masks.append(masks[index])\n",
        "    return np.array(result_masks)\n",
        "\n",
        "def upscale_nearest_neighbor(image, scale_factor):\n",
        "    \"\"\"Upscale an image using nearest-neighbor interpolation.\"\"\"\n",
        "\n",
        "    height, width, channels = image.shape\n",
        "    new_height = int(height * scale_factor)\n",
        "    new_width = int(width * scale_factor)\n",
        "\n",
        "    upscaled_image = np.zeros((new_height, new_width, channels), dtype=image.dtype)\n",
        "\n",
        "    for y in range(new_height):\n",
        "        for x in range(new_width):\n",
        "            orig_y = int(y / scale_factor)\n",
        "            orig_x = int(x / scale_factor)\n",
        "            upscaled_image[y, x] = image[orig_y, orig_x]\n",
        "\n",
        "    return upscaled_image\n",
        "\n",
        "def annotate_image(image, classes, detections):\n",
        "    height, width = image.shape[:2]\n",
        "    scale_factor = min(width, height)\n",
        "    text_scale = 0.001 * scale_factor\n",
        "\n",
        "    box_annotator = sv.BoxAnnotator()\n",
        "    mask_annotator = sv.MaskAnnotator()\n",
        "    label_annotator = sv.LabelAnnotator(text_scale=text_scale, text_thickness=3)\n",
        "\n",
        "    labels = [\n",
        "        f\"{classes[class_id]} {confidence:0.2f}\"\n",
        "        for _, _, confidence, class_id, _, _\n",
        "        in detections\n",
        "    ]\n",
        "    annotated_image = mask_annotator.annotate(scene=image.copy(), detections=detections)\n",
        "    annotated_image = box_annotator.annotate(scene=annotated_image, detections=detections)\n",
        "    annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections, labels=labels)\n",
        "\n",
        "    %matplotlib inline\n",
        "    sv.plot_image(annotated_image, (16, 16))\n",
        "\n",
        "    return annotated_image\n",
        "\n",
        "def segment_image(image, classes, box_threshold, text_threshold):\n",
        "    detections = grounding_dino_model.predict_with_classes(\n",
        "      image=image,\n",
        "      classes=enhance_class_name(class_names=classes),\n",
        "      box_threshold=box_threshold,\n",
        "      text_threshold=text_threshold\n",
        "    )\n",
        "\n",
        "    detections = detections[detections.class_id != None]\n",
        "    detections.mask = segment(\n",
        "        sam_predictor=sam_predictor,\n",
        "        image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB),\n",
        "        xyxy=detections.xyxy\n",
        "    )\n",
        "\n",
        "    return detections\n",
        "\n",
        "##\n",
        "\n",
        "def segment_original_image(img_input, query, box_threshold, text_threshold):\n",
        "    image = np.array(img_input)\n",
        "    if image.shape[2] == 4:\n",
        "        image = image[:, :, :3]\n",
        "\n",
        "    CLASSES = query.split()\n",
        "    BOX_THRESHOLD = box_threshold\n",
        "    TEXT_THRESHOLD = text_threshold\n",
        "\n",
        "    detections = segment_image(image, CLASSES, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
        "\n",
        "    annotated_image = annotate_image(image, CLASSES, detections)\n",
        "    return annotated_image\n",
        "\n",
        "##\n",
        "\n",
        "def segment_selected_image(img_input, query, box_threshold, text_threshold, selected_pos=None):\n",
        "    image = np.array(img_input)\n",
        "    if image.shape[2] == 4:\n",
        "        image = image[:, :, :3]\n",
        "\n",
        "    CLASSES = query.split()\n",
        "    BOX_THRESHOLD = box_threshold\n",
        "    TEXT_THRESHOLD = text_threshold\n",
        "\n",
        "    detections = segment_image(image, CLASSES, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
        "\n",
        "    selected_detections = []\n",
        "    if selected_pos is not None:\n",
        "      for xyxy, _, _, _, _, _ in detections:\n",
        "        if int(float(xyxy[0].astype(float))) <= selected_pos[0] <= int(float(xyxy[2].astype(float))) and int(float(xyxy[1].astype(float))) <= selected_pos[1] <= int(float(xyxy[3].astype(float))):\n",
        "          selected_detections.append(xyxy)\n",
        "\n",
        "    selected_xyxy = None\n",
        "    selected_size = 0\n",
        "    if len(selected_detections) != 0:\n",
        "      for xyxy in selected_detections:\n",
        "        if selected_xyxy is None:\n",
        "          selected_xyxy = xyxy\n",
        "        else:\n",
        "          curr_size = (xyxy[2] - xyxy[0]) * (xyxy[3] - xyxy[1])\n",
        "          min_size = (selected_xyxy[2] - selected_xyxy[0]) * (selected_xyxy[3] - selected_xyxy[1])\n",
        "          if curr_size < min_size:\n",
        "            selected_xyxy = xyxy\n",
        "            selected_size = curr_size\n",
        "\n",
        "    if selected_xyxy is not None:\n",
        "      image = image[int(float(selected_xyxy[1]))-25:int(float(selected_xyxy[3]))+25,\n",
        "              int(float(selected_xyxy[0]))-25:int(float(selected_xyxy[2]))+25, :3]\n",
        "      image = upscale_nearest_neighbor(image, 5)\n",
        "      detections = segment_image(image, CLASSES, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
        "\n",
        "      annotated_image = annotate_image(image, CLASSES, detections)\n",
        "      return image, annotated_image\n",
        "\n",
        "    annotated_image = annotate_image(image, CLASSES, detections)\n",
        "    return image, annotated_image\n",
        "\n",
        "##\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "css = \"\"\"\n",
        "/* CSS to make the image scale dynamically */\n",
        ".gr-box .gr-image {\n",
        "    width: 100%;\n",
        "    height: auto;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=css) as demo:\n",
        "    gr.Markdown(\"# Segment Anything Model\")\n",
        "\n",
        "    with gr.Column():\n",
        "        with gr.Row(equal_height=True):\n",
        "            box_threshold = gr.Slider(minimum=0.0, maximum=1.0, value=0.35, step=0.01, label=\"Box Threshold\")\n",
        "            text_threshold = gr.Slider(minimum=0.0, maximum=1.0, value=0.25, step=0.01, label=\"Text Threshold\")\n",
        "        textbox = gr.Textbox(lines=1, show_label=False, placeholder=\"What objects would you like to segment? (e.g. car, dog, ear)\")\n",
        "\n",
        "    with gr.Row(equal_height=True):\n",
        "        image_input = gr.Image(type=\"pil\", label=\"Upload an Image\")\n",
        "        image_output = gr.Image(type=\"pil\", label=\"Segmented Image\")\n",
        "\n",
        "    def on_input_change(image, query, box_threshold, text_threshold):\n",
        "        if image is not None:\n",
        "            return segment_original_image(image, query, box_threshold, text_threshold)\n",
        "        return None\n",
        "\n",
        "    def on_img_select(image, query, box_threshold, text_threshold, evt: gr.SelectData):\n",
        "        selected_pos = evt.index\n",
        "        return segment_selected_image(image, query, box_threshold, text_threshold, selected_pos)\n",
        "\n",
        "    image_input.change(on_input_change, inputs=[image_input, textbox, box_threshold, text_threshold], outputs=image_output)\n",
        "    textbox.change(on_input_change, inputs=[image_input, textbox, box_threshold, text_threshold], outputs=image_output)\n",
        "    box_threshold.change(on_input_change, inputs=[image_input, textbox, box_threshold, text_threshold], outputs=image_output)\n",
        "    text_threshold.change(on_input_change, inputs=[image_input, textbox, box_threshold, text_threshold], outputs=image_output)\n",
        "\n",
        "    image_output.select(on_img_select, inputs=[image_input, textbox, box_threshold, text_threshold], outputs=[image_input, image_output])\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "hB3GJVHEoqRY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}